Eines der wichtigsten und am meisten erforschten Felder in der Entwicklung autonomer Systeme ist die Vermeidung von Hindernissen in Echtzeit. Die wesentliche Grundlage dafür besteht in der Erkennung der Hindernisse. Dies geschieht oft mit Hilfe optischer Messtechniken.\\
\noindent
Im folgenden Kapitel werden einige State of the Art Algorithmen erläutert. Dabei werden zunächst einige aktive optische Systeme näher beschrieben, bei denen die betrachtete Szene aktiv ausgestrahlt wird. Anschließend erfolgt die Analyse weiterer passiver optischer Algorithmen welche auf der Berechnung von Disparity Maps unter Zuhilfenahme stereo optischer Systeme basiert. Die Kalkulation dieser ist generell sehr rechenaufwändig, liefert aber nach der Auswertung zuverlässige Informationen über die Entfernung sowie Position des aktuellen Objektes.


% ---------------------- section -----------------------
\section{Aktiv optische Algorithmen}
\label{sec:kamera_basierte_he}
% ToDo: quellen aktiv optische algorithmen
Aktiv optische Algorithmen beziehen sich auf die aktive Emission von Licht auf die zu rekonstruierende Szene. Dies geschieht indem beispielsweise Muster auf die Szene projiziert werden (Gitter, Streifen, Farben, etc.), mit deren Hilfe es möglich ist korrespondierende Pixel innerhalb des Bildes zu finden. Jedoch können bei uniformen beziehungsweise einfachen Mustern falsche Bereiche des Musters erkannt werden und somit eine falsche räumliche Repräsentation der Szene. Ein alternativer Ansatz ist die Projektion zufälliger Muster um etwaige falsche Korrespondenzen durch bewusste Zufälligkeit zu vermeiden. Ein weiteres Feld in der Betrachtung aktiver visueller Techniken ist Time of Flight. Hierbei wird die Szene mit einem Lichtimpuls (meist infrarot) ausgeleuchtet und für jeden Pixel die Zeit gemessen die jener benötigt um wieder auf dem Sensor aufzutreffen. Damit ist es möglich in geringer Auflösung exakte Tiefendaten für jedes Einzelbild zu erhalten.\\

\noindent
Die von Lee et al. \cite{lee2012intelligent} verfolgte Methodik bedient sich einer Time of Flight Kamera. Die aufgenommenen Tiefenbilder werden in verschiedene Segmente eingeteilt welche die verschiedenen Objekte repräsentieren. Sobald etwaige Kanten gefunden wurden werden diese entfernt unter dem Ansatz die Objekte so leichter segmentieren zu können. Anschließend erfolgt eine Tiefenanalyse der nun vorliegenden einzelnen Segmente. Zum Finden der Hindernisse wird nun die Standardabweichung jedes Segmentes berechnet.
%TODO Standardabweichung Formel?!
Dabei gelten Hindernisse als sich bewegende oder statische Objekte welche sich innerhalb eines definierten Gefahrenbereichs befinden. Lee et al. definieren Ihre Gefahrenzone dabei mit $1-2$ Metern. Die erkannten Hindernisse werden nun mit Ihrer Distanz markiert. Ein häufig auftretendes Problem in diesem Ansatz spiegelt der Boden wieder, welcher oft als Hindernis erkannt wird. Ein Mittel zur Unterscheidung ist hierbei ebenfalls die Standardabweichung.\\

\noindent
Bei der Entwicklung eines autonomen Roboters zur Indoor-Überwachung verschiedener Areale bedienen sich Correa et al. \cite{correa2012mobile} eines Microsoft Kinect Sensors zur Erstellung von Disparity Maps. Dabei werden diese ebenfalls wie bereits in diversen passiven Algorithmen (\cite{pire2012stereo}, \cite{kostavelis2010comparative}) in mehrere horizontale Segmente eingeteilt. Dabei besteht die finale Karte aus 5 Bereichen, von denen 3 als mögliche Richtungen betrachtet werden. Sobald die die minimalste Distanz eines Sektors geringer als 60cm ist, so wird angenommen, dass sich das System vor einem Hindernis befindet. Ausgehend von der Menge an Sektoren mit gefunden Hindernissen wird die neue Bewegungsrichtung angepasst.

% ---------------------- section -----------------------
\section{Passiv optische Algorithmen}
\label{sec:sensor_basierte_he}
% ToDo: quellen passiv optische algorithmen
Passiv optische Algorithmen beziehen sich auf die Erfassung dreidimensionaler Informationen aus der Szene ohne eigene Lichtquelle. Dabei sind viele dieser Methoden an das menschliche oder auch tierische Sehen angelehnt. Ein großer Teil der Methodik ist das Prinzip Stereo Vision welches durch die Differenz zweier Bilder derselben Szene einen Eindruck von Tiefe verschafft. Weiterhin kann eine dreidimensionale Rekonstruktion der Umgebung durch Lichteinflüsse, Schattierung oder durch Veränderung des Kamerafokus vorgenommen werden.\\

\noindent
Bei der Entwicklung eines autonomen Roboters auf Bodenebene wird nach Kostavelis et al, \cite{kostavelis2010comparative} die errechneten Disparity Maps in 3 horizontale Bildbereiche aufgeteilt, welche die möglichen Richtungen des Systems beschreiben. Für jedes dieser einzelnen Unterbilder wird nun der Durchschnitt der Disparität berechnet, wobei der Bildteil mit dem geringsten Wert auf Hindernisse hinweist, welche sich näher am System befinden. Aufgrund dessen, dass die Entscheidung welcher Weg nun der als am sichersten zu betrachten ist, zu Teilen sehr schwer zu treffen ist wurde die Threshold Estimation Methode entwickelt.
%TODO: nochmal nachlesen
Die Unterteilung der Bilder wir in diesem Algorithmus ebenfalls in drei Regionen vorgenommen. Zunächst werden alle Pixel deren Wert sich über einem vordefinierten Threshold befinden markiert, und gezählt. Wenn die Anzahl der als Hindernis definierten Pixel über einem ebenfalls vor definiertem Prozentsatz liegen, so wird ein Hindernis als gefunden markiert (in der jeweiligen Region). Die letzte vorgestellte Methode von Kostavelis et al. orientiert sich in ihrer Funktionsweise an der soeben beschriebenen Threshold Estimation und erweitert den Algorithmus um die Betrachtung aller 3 Bildteile.
Bei der multi Threshold Estimation wird jedes Drittel des Bildes betrachtetet, wobei in allen Regionen nach Pixeln innerhalb des Thresholds gesucht und markiert werden. Anschließend werden die Ergebnisse untereinander verglichen, und das Drittel mit dem geringsten Wert als Hindernis ausgewählt. Sollten die prozentualen Werte aller Drittel größer sein als die gegebene Grenze so wird angenommen das sich das Hindernis sehr nah vor dem System befindet.

Im Rahmen dieser Arbeit wurde ebenfalls eine Methode entwickelt welche sich grob am ersten Ansatz orientiert. Diese wird in Kapitel \ref{chp:developed_algorithms} erläutert.\\

\noindent
Einen anderen Ansatz zur Erkennung von Hindernissen geben Richards et al. \cite{richards2014obstacle} . Der Algorithmus wurde als Hindernisvermeidung für ein UAV entwickelt. Optischer Fluss sowie Feature Tracking bieten dabei die Grundlage für die Erkennung, Vermeidung und Voraussage welcher Bereich der sicherste ist. Dabei gehen die beiden Ausgangstechniken jeweils einer anderen Aufgabe nach. Der Optische Fluss dient zum Erkennen und verfolgen von Objekten sowie für die Voraussage der Position des Objektes im nächsten Einzelbild, wohingegen das Feature Tracking \cite{shi1994good} für die Erkennung von markanten Punkten innerhalb des Objektes genutzt wird. Bei diesem wird in jedem folgenden Frame verglichen, ob bereits bekannte Punkte innerhalb einer bestimmten Distanz mit denen des aktuellen Frames übereinstimmen. Zur Abschätzung der nächsten Position des Features wird mit Hilfe des Optische Fluss der  Verschiebungsvektor zwischen beiden Frames berechnet. Aufgrund dessen, dass der zugrunde liegende Algorithmus von Lukas-Kanade \cite{lucas1981iterative} nur bei kleinen Verschiebungen valide Ergebnisse liefert, wird ein pyramidaler Ansatz \cite{bouguet2001pyramidal} für das Matching verwendet bei welchem die jeweiligen Bilder des Frames herunter skaliert werden. Durch die Verbindung dieser beiden Techniken, lassen sich Objekte erkennen und verfolgen. Zur Bestimmung der nächstbesten Position für das UAV wird ausgehend von den berechneten Resultaten (gefundene und erkannte Hindernisse) eine stochastische Matrix erstellt welche zur weiteren Planung des Fluges verwendet wird.\\

\noindent
% TODO: Einleitung aendern
Bei der Entwicklung von Systemen, welche auf der Erkennung räumlicher Tiefe basieren, bieten stereo optische Systeme einen großen Vorteil. Durch die Verschiebung der Kameras an der Basislinie wird die Szene aus zwei minimal verschiedenen Blickwinkeln aufgenommen. Dies ermöglicht die Berechnung dreidimensionaler Informationen auf einfache Weise. Trotz dessen ist die Benutzung zweier Kameras nicht immer möglich, sei es die Limitierung durch das System selber aus Platzgründen oder, im Falle unbemannter Flugsysteme, die begrenzte maximale Traglast. Aufgrund letzterer entschieden sich Mori et al. \cite{mori2013first} für die Nutzung eines monokularen Setups. Durch die Verwendung des optischen Flusses ist es auch mit nur einer Kamera möglich Hindernisse zu erkennen, jedoch fehlt die eigentliche Erkennung von Tiefe. Sofern sich ein Objekt direkt auf das System zu bewegt, kann es nur schwer erkannt werden, da kaum perspektivische Veränderungen vorhanden sind. Um diese wahrzunehmen muss der Algorithmus dazu in der Lage sein, die Veränderung der relativen Größe eines Objektes in aufeinander folgenden Bildern abzuschätzen. Mori et al. setzen bei der Erkennung von Features auf den SURF Algorithmus \cite{bay2006surf} welcher gefundene Features auch nach Veränderung der Größe wieder erkennen kann. Im Anschluss daran wird die Veränderung der Größe der benachbarten Umgebung untersucht um eine Aussage darüber treffen zu können ob sich das Hindernis auf das System zu bewegt. Jene Features welche nicht skaliert wurden, werden nicht weiter betrachtet. In jedem folgenden Einzelbild wird nun mithilfe von Template Matching verglichen wie sich die Skalierung der Umgebung eines Features im Vergleich zum vorherigen Frame verändert hat. Sollte die Distanz eines Features zu nah am System sein, so wird dieses als potentielles Hindernis für die Vermeidung verwendet.
