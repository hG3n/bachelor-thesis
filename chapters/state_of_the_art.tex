Eines der wichtigsten und am meisten erforschten Felder in der Entwicklung autonomer Systeme ist die Vermeidung von Hindernissen in Echtzeit. Die wesentliche Grundlage dafür besteht in der Erkennung der Hindernisse. Dies geschieht oft mit Hilfe optischer Messtechniken.\\

\noindent
Im folgenden Kapitel werden einige State of the Art Verfahren erläutert. Dabei werden zunächst verschiedene aktive optische Systeme näher beschrieben, bei denen die betrachtete Szene aktiv ausgeleuchtet wird. Anschließend erfolgt die Analyse weiterer passiver optischer Algorithmen, welche auf der Berechnung von sogenannten Disparitätenkarten unter Zuhilfenahme stereo-optischer Systeme basiert. Diese berechneten Tiefenkarten enthalten die horizontale Verschiebung korrespondierender Bereiche in beiden Bildern. Die Kalkulation solcher ist generell sehr rechenaufwändig, liefert aber nach der Auswertung Informationen über die Entfernung sowie Position der abgebildeten Szene. Die dabei erhaltenen Ergebnisse hängen von dem jeweils verwendeten \emph{Stereo-Matching} Algorithmus ab.

% ---------------------- section -----------------------
\section{Aktiv optische Algorithmen}
\label{sec:kamera_basierte_he}
% ToDo: quellen aktiv optische algorithmen
Aktiv optische Algorithmen basieren auf einer aktiven Beleuchtung der zu rekonstruierenden Szene. Dies geschieht indem beispielsweise Muster auf die Szene projiziert werden (Gitter, Streifen, Farben, etc.), mit deren Hilfe es möglich ist, das Korrespondenzproblem, also das finden korrespondierender Punkte in zwei Bildern, einfach und robust zu lösen. Uniforme bzw. einfache Muster führen zu Mehrdeutigkeiten, die leicht Fehlzuordnungen herbeiführen können. Ein alternativer Ansatz ist die Projektion zufälliger Muster, um etwaige falsche Korrespondenzen durch bewusste Zufälligkeit zu vermeiden. Ein weiteres Feld in der Betrachtung aktiver visueller Techniken ist \emph{time of flight} (TOF). Hierbei wird die Szene mit einem Lichtimpuls (meist infrarot) ausgeleuchtet und für jeden Pixel die Zeit gemessen, die jener benötigt, um wieder auf dem Sensor aufzutreffen. Damit ist es möglich, in geringer Auflösung dichte Tiefendaten für jedes Einzelbild zu messen.\\

\noindent
Die von Lee et al. \cite{lee2012intelligent} vorgeschlagene Methodik der Hinderniserkennung bedient sich einer TOF Kamera. Die aufgenommenen Tiefenbilder werden basierend auf den Tiefenmessungen segmentiert. Dabei werden mi Hilfe eines Algorithmus zur Kantenerkennung jene herausgefiltert und anschliessend von der Tiefenkarte subtrahiert. Anschließend erfolgt eine Tiefenanalyse der nun vorliegenden einzelnen Segmente. Um die gefundenen Segmente als Hindernis klassifizieren zu können wird zunächst die Standardabweichung jedes Segements berechnet. Da die Tiefenwerte des Bodens einen hohen Grad der Streuung aufweisen ist die Standardabweichung dessen höher als bei anderen Segmenten. Diese Information dient zur Unterscheidung zwischen Hindernis und Boden. Dabei gelten Hindernisse als sich bewegende oder statische Objekte, welche sich innerhalb eines definierten Gefahrenbereichs befinden. Lee et al. definieren Ihre Gefahrenzone dabei mit $1-2$ Metern. Die erkannten Hindernisse werden nun anhand der mittleren Distanz markiert. Ein häufig auftretendes Problem in diesem Ansatz spiegelt der Boden wieder, welcher oft als Hindernis erkannt wird. Ein Mittel zur Unterscheidung ist hierbei ebenfalls die Standardabweichung.\\

\noindent
Bei der Entwicklung eines autonomen Roboters zur Indoor-Überwachung verschiedener Areale bedienen sich Correa et al. \cite{correa2012mobile} eines Microsoft Kinect Sensors zur Erstellung von Disparitätenkarten. Dabei werden diese ebenfalls, wie bereits in diversen passiven Algorithmen (\cite{pire2012stereo}, \cite{kostavelis2010comparative}) in mehrere horizontale Segmente eingeteilt. Die finale Karte besteht aus 5 Bereichen, von denen 3 als mögliche Richtungen betrachtet werden. Sobald die die minimalste Distanz eines Sektors geringer als 60 cm ist wird angenommen, dass sich das System vor einem Hindernis befindet. Ausgehend von der Menge an Sektoren mit gefunden Hindernissen wird die neue Bewegungsrichtung angepasst.

% ---------------------- section -----------------------
\section{Passiv optische Algorithmen}
\label{sec:sensor_basierte_he}
% ToDo: quellen passiv optische algorithmen
Passiv optische Algorithmen beziehen sich auf die Berechnung dreidimensionaler Informationen aus Bildern ohne eigene Lichtquelle. Dabei sind viele dieser Methoden an das menschliche oder auch tierische Sehen angelehnt. Ein großer Teil der Methodik ist das Prinzip Stereo Vision welches durch die Differenz zweier Bilder derselben Szene einen Eindruck von Tiefe verschafft. Weiterhin kann eine dreidimensionale Rekonstruktion der Umgebung durch die Analyse von Bildern hinsichtlich Lichteinflüssen, Schattierungen oder durch Veränderung des Kamerafokus vorgenommen werden.\\

\noindent
Bei der Entwicklung eines autonomen Roboters auf Bodenebene werden nach Kostavelis et al, \cite{kostavelis2010comparative} die errechneten Disparitätenkarten in 3 horizontale Bildbereiche aufgeteilt, welche die möglichen Bewegungsrichtungen des Systems repräsentieren. Für jedes dieser einzelnen Unterbilder wird nun der Durchschnitt der Disparitäten berechnet, wobei der Bildteil mit dem geringsten Wert auf Hindernisse hinweist, welche sich näher am System befinden. Angesichts der Tatsache, dass die Entscheidung darüber, welcher Weg als der sicherste angesehen werden muß, teilweise schwer zu treffen ist, wurde die sogenannte \emph{Threshold Estimation} Methode entwickelt. Die Unterteilung der Bilder wird hier ebenfalls in drei Regionen vorgenommen. Zunächst werden alle Pixel deren Werte sich über einem vordefinierten Schwellwert befinden markiert und gezählt. Wenn die Anzahl der als Hindernis definierten Pixel über einem ebenfalls vordefinierten Prozentsatz liegen, so wird ein Hindernis als gefunden markiert (in der jeweiligen Region). Die letzte vorgestellte Methode von Kostavelis et al. orientiert sich in ihrer Funktionsweise an der soeben beschriebenen Schwellwertschätzung und erweitert den Algorithmus um die Betrachtung aller 3 Bildteile. Bei der \emph{multi threshold estimation} wird jedes Drittel des Bildes betrachtetet, wobei in allen Regionen nach Pixeln innerhalb des Schwellwerts gesucht und diese markiert werden. Anschließend werden die Ergebnisse untereinander verglichen, und das Drittel mit dem geringsten Wert als Hindernis ausgewählt. Sollten die prozentualen Werte aller Drittel größer sein als die gegebene Grenze so wird angenommen das sich das Hindernis sehr nah vor dem System befindet.\\

\noindent
Eine weitere Methode zur Hindernisvermeidung für UAVs wurde von Richards et al. \cite{richards2014obstacle} vorgestellt. Optischer Fluss sowie \emph{Feature Tracking} bieten dabei die Grundlage für die Erkennung, Vermeidung und Voraussage, welcher Bereich im Raum der sicherste ist. Dabei gehen die beiden Ausgangstechniken jeweils einer anderen Aufgabe nach. 
Der Optische Fluss dient zum Erkennen und Verfolgen von Objekten sowie für die Voraussage der Position im nächsten Einzelbild, wohingegen das \emph{Feature Tracking} \cite{shi1994good} für die Erkennung von markanten Punkten innerhalb des Objektes genutzt wird. Bei diesem wird in jedem folgenden Einzelbild verglichen, ob bereits bekannte Punkte innerhalb einer bestimmten Distanz mit denen des aktuellen Bildes übereinstimmen. Zur Schätzung zu erwartenden Position der Merkmale im darauffolgenden Bild werden mit Hilfe des Optische Fluss’ die Verschiebungsvektoren zwischen beiden Bildern berechnet. Aufgrund dessen, dass der zugrunde liegende Algorithmus von Lukas-Kanade \cite{lucas1981iterative} nur bei kleinen Verschiebungen valide Ergebnisse liefert, wird ein pyramidaler Ansatz \cite{bouguet2001pyramidal} für das Matching verwendet, bei welchem beide Bilder eines \emph{frames} herunter skaliert werden. Durch die Verbindung dieser beiden Techniken lassen sich Objekte erkennen und verfolgen. Zur Bestimmung der nächstbesten Position für das UAV wird ausgehend von den berechneten Resultaten (gefundene und erkannte Hindernisse) eine stochastische Matrix erstellt welche zur weiteren Planung des Fluges verwendet wird.\\

\noindent
Bei der Entwicklung von Algorithmen, welche auf der Erkennung räumlicher Tiefe basieren, bieten stereo-optische Systeme einen großen Vorteil. Durch die Verschiebung der Kameras auf der Basislinie wird die Szene aus zwei minimal verschiedenen Blickwinkeln aufgenommen. Dies ermöglicht die Berechnung dreidimensionaler Informationen einerseits mithilfe verschiedenster Feature Tracking Methoden sowie anschließender Triangulierung oder andrerseits unter Zuhilfenahme diverser Algorithmen zur Lösung des Stereo-Korrespondez Problemes.\\
\noindent
Trotzdessen ist die Benutzung zweier Kameras nicht immer möglich, sei es durch die Limitierung durch das System selber aus Platzgründen oder, im Falle unbemannter Flugsysteme, durch die begrenzte maximale Traglast. Aufgrund letzterer entschieden sich Mori und Scherer \cite{mori2013first} für die Nutzung eines monokularen Setups. Durch die Verwendung des optischen Flusses ist es auch mit nur einer Kamera möglich, Hindernisse zu erkennen, jedoch fehlt die eigentliche Erkennung von Tiefe. Sofern sich ein Objekt direkt auf das System zu bewegt, kann es nur schwer erkannt werden, da kaum perspektivische Veränderungen vorhanden sind. Um diese wahrzunehmen, muss der Algorithmus dazu in der Lage sein, die Veränderung der relativen Größe eines Objektes in aufeinander folgenden Bildern abzuschätzen. Mori und Scherer setzten bei der Erkennung von Merkmalen auf den SURF Algorithmus \cite{bay2006surf}, welcher gefundene Features auch nach Skaleninvarianz wieder erkennen kann. Im Anschluss daran wird die Veränderung der Größe der benachbarten Umgebung untersucht, um eine Aussage darüber treffen zu können, ob sich das Hindernis auf das System zu bewegt. Jene Features welche nicht skaliert wurden, werden nicht weiter betrachtet. In jedem folgenden Einzelbild wird nun mit Hilfe von \emph{Template Matching} verglichen, wie sich die Skalierung der Umgebung eines Features im Vergleich zum vorheringen Frame verändert hat. Sollte die Distanz eines Features zu nah am System sein, so wird dieses als potentielles Hindernis für die Vermeidung verwendet.
