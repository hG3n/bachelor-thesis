In der Erkennung von Hindernissen mithilfe von passiv optischen Systemen können verschiedene Faktoren der Grund für eine fehlerhafte Erkennung sein. Sei es die Berechnung einer Disparitätenkarte von Bereichen mit einer Vielzahl homogener oder reflektierender Flächen oder die Veränderung der Lichtverhältnisse in einem der beiden Kamerabilder. In diesem Kapitel werden einige der bestehenden, sowie einige potentiell mögliche Limitierungen erläutert sowie dazugehörige Lösungsansätze entwickelt. Weiterhin werden diese in Abschnitt \ref{sec:existing_conflicts} diskutiert. Zudem werden sowohl eigene, als auch literarisch belegte Lösungsansätze präsentiert\\

% ---------------------- section -----------------------
\section{Bestehende Limitierungen und Lösungsansätze}
\label{sec:existing_conflicts}

Die Validierung der erkannten Hindernisse ist ein kompliziertes Problem in der autonomen Hinderniserkennung. Durch etwaige äußere Einflüsse wie die Veränderung der Lichtverhältnisse kann die Berechnung der Tiefenkarte fehlerhaft sein. Dies kann im Fall eines autonomen Flugs dazu führen, dass das UAS ein Hindernis innerhalb eines Korridors erkennt und aufgrund dessen versucht diesem imaginären Hindernis auszuweichen. Gerade in engen Umgebungen ohne viel verfügbaren Platz kann dies zum totalen Systemausfall führen. Selbiges gilt für nicht erkannte Hindernisse. Daher sollte ausgewertet werden ob die erkannten Hindernisse Resultat fehlerhafter Disparitätenkarten sind. Ein Ansatz zur Vermeidung solcher Falscherkennungen wäre, die Objekte insofern zu verfolgen, dass für jeden Frame (in Abhängigkeit der zugrundeliegenden Framerate) überprüft wird, ob im vorherigen Frame bereits ein Objekt gefunden wurde. Erst nachdem dies sichergestellt wurde, wird daraufhin eine Warnung ausgegeben. Eine Möglichkeit der Vorhersage der Position von bereits detektierten Hindernissen in früheren Frames ist durch den Einbezug des Wissens über die Eigenbewegung der Drohne möglich.\\

\noindent
Eine andere Problematik stellt die Erkennung von homogenen, spiegelnden sowie durchsichtigen Flächen dar. Aufgrund fehlender Texturen sowie vorhandener Pixeldifferenzen kann die Korrespondenz zweier Pixel unter Umständen nicht bestimmt werden. Diese homogenen Bereiche haben demnach fehlerhafte Informationen zur Folge, so kann einem Pixel an einer weißen Wand kein zugehöriger Pixel zugeordnet werden. Außerdem besteht die Möglichekeit das falsche Bildpunkte miteinander gematcht werden welche wiederum falsche Disparitäten zur Folge haben. Abbildung \ref{fig:disparity-error-homogeneous} zeigt deutlich, in welchen Bereichen der SGBM aufgrund fehlender Textur keine Korrespondenzen finden konnte.\\

\begin{figure}[h]
	\centering
	\begin{tabular}{m{6.5cm} m{6.5cm}}
	\includegraphics[width=6.5cm]{img/disparity_error_left.pdf}
	\begin{center} \small (a) linkes Kamerabild \end{center}
	&
	\includegraphics[width=6.5cm]{img/disparity_error.pdf}
	\begin{center} \small (b) Disparity Map \end{center}
	\end{tabular}
    \caption{Konflikt in der Berechnung der Disparität bei nicht texturierten Flächen. Dabei ist (a) das linke aufgenommene Kamerabild und (b) die dazu gehörige Disparitätenkarte. Die in rot markierten Flächen sind das Resultat homogener Texturen.}
    \label{fig:disparity-error-homogeneous}
\end{figure}

\noindent
Auch spiegelnde Flächen sind eine optische Problemstellung. Aufgrund der Projektion eines anderen Bereiches kommt es zu falschen Informationen innerhalb der Disparitätenkarte. Hindernisse können zwar erkannt werden sofern sich die spiegelnde Fläche in einer günstigen Position befindet. Dies ist der Fall, wenn beide Kameras exakt den selben Bereich erfassen. Jedoch wird auch dabei anstelle des eigentlichen Objektes eine weiter entfernte Distanz wahrgenommen und somit ein Hindernis, welches sich an einer gänzlich anderen Stelle befindet. Ist dies nicht der Fall so kann auch im Fall spiegelnder Flächen keine Disparität und folglich keine Tiefe wahrgenommen werden.\\
    
\noindent
Eine weitere Fehlerquelle sind durchsichtige Bereiche. Diese vereinen zum einen Fehlerquellen, welche auch bei spiegelnden Arealen auftreten, zum anderen werden Objekte, die sich hinter einer Glasscheibe befinden zwar erkannt (sofern keine Reflexion vorliegt), jedoch das Glas als eigentliches Hindernis nicht. Weiterhin besteht die Möglichkeit, dass Reflexionen aus Perspektive der beiden Kameras betrachtet einen anderen Tiefeneindruck vermitteln. Objekte werden so entweder weiter entfernt oder auch in kürzerer Distanz erkannt als sie sich wirklich befinden.\\

\noindent
Aufgrund des Aspektes, dass das System für den Gebrauch in Innenbereichen konzipiert ist, gibt es verschiedenste Konflikte bei der Anwendung in Außenbereichen. Zum einen ist die Verschlusszeit der Kameras fixiert. Die Verwendung einer automatischen Belichtungszeit ist nur für jede Kamera einzeln möglich. Um mit verschiedenen Lichtverhältnissen umgehen zu können, ohne dabei die Möglichkeit der Korrespondenzanalyse zu verlieren, muss die Verschlusszeit daher bei beiden Kameras immer gleich sein. Weiterhin ist die Erkennung des Bodens ein zu betrachtender Aspekt. Mit den vorgeschlagenen Methoden wird der Boden aktuell als Hindernis erkannt, was einerseits seine Berechtigung hat, andererseits auch bei niedrigen Flughöhen zu einer fehlerhaften Erkennung als Hindernis führt.\\

\noindent
Die Erkennung sehr kleiner Hindernisse ist aufgrund verschiedenster Faktoren deutlich eingeschänkt. Ist das Hindernis so klein, dass es nur Teile eines \emph{Subimages} oder \emph{Samplepoints} ausfüllt, so ist die Erkennung dessen sehr stark vom Hintergrund abhängig. Dies resultiert aus der Berechnung des Mittelwertes zur Erkennung der Hindernisse. Ist der Hintergrund sehr weit entfernt beeinflusst dieser die Summe aller Pixel eines \emph{Subimages}/\emph{Samplepoints} insofern, dass die gemittelte Disparität nicht mehr innerhalb der Gefahrenzone liegt. 


% ---------------------- section -----------------------
\section{Diskussion}
\label{sec:conflict_discussion}
Ein Ansatz, erkannte Hindernisse zu validieren ist, sich auf die letzte bekannte Position des Objektes zu berufen und im nächsten Einzelbild zu überprüfen, ob es sich noch immer an derselben Position befindet. Dabei würde für jedes \emph{Subimage}/ jeden \emph{Samplepoint} ausgewertet werden, ob sich das Hindernis noch innerhalb dessen befindet. Jedoch schränkt dies die Erkennung bewegter Objekte stark ein. Ist die Geschwindigkeit eines Hindernisses so hoch, das in mindestens zwei aufeinanderfolgenden Frames kein Objekt im selben \emph{Subimage}/\emph{Samplepoint} erkannt werden kann so würde das System kein Hindernis erkennen können. Diese Methode ist daher prinzipiell mögliche, jedoch stark von der Framerate sowie der eigenen bzw. Bewegung des Objektes abhängig. Grundlegend ist das System aufgrund der Beschaffenheit der \emph{Subimages} für die \emph{Subimage Detection} geeignet, da die Größe der \emph{Subimages} eine solche Validierung zulassen würde. Im Falle der \emph{Samplepoint Detection} müssten auch die unmittelbar benachbarten Messpunkte mit einbezogen werden sodass die Bewegung auch verfolgt werden kann. Eine Kombination von Feature Tracking wäre dabei hilfreich, jedoch sehr rechenaufwändig.\\
	% paper suchen

\noindent
Um auch die Erkennung homogener Flächen gewährleisten zu können, besteht die Möglichkeit, die Szene mithilfe externer Lichtquellen auszuleuchten um eine Texturierung zu erzwingen. Mit Hilfe eines Lasers könnte beispielsweise ein zufälliges Linienmuster auf die Szene projiziert werden, durch welches es dem Matching Algorithmus möglich ist, korrespondierende Punkte innerhalb eines nicht texturierten Bereiches zu finden. Die Zufälligkeit des Musters ist dabei ein wichtiges Kriterium, da die Gleichmäßigkeit eines Rasters zu falschen Korrespondenzen führen könnte. Die Anwendung dieses Verfahrens ist jedoch hauptsächlich in Innenbereichen möglich, da die zu erkennenden Entfernungen unter Betrachtung der Einzelbild-Auflösung eine Detektion dieser zulassen würden.

\noindent
In vielen Fällen sind gefundene Stereo Korrespondenzen nicht eindeutig, so beschreiben Geiger et al. \cite{geiger2011efficient} die Berechnung solcher im ELAS (\emph{Efficient Large-scale Stereo}) Algorithmus. Zu Beginn des Matching Verfahrens wird eine karge Menge an Hilfspunkten berechnet. Diese sind dabei als Pixel definiert, welche aufgrund der gegebenen Textur oder ihrer Einzigartigkeit robust gematcht werden können. Dabei werden solche \enquote{Support Points} als eine Kon­ka­te­na­ti­on der Koordinaten des Pixels $(x_m,y_m)$ sowie der zugehörigen Disparität $d_m$ definiert. Die Verteilung der Hilfspunkte auf den Referenzbildern erfolgt in gleichem Abstand zueinander. Unklare Matches werden währenddessen anhand eines Schwellwertes aussortiert. Anschliessend wird unter Zuhilfenahme der erstellten Hilfspunkte nach sogenannten \emph{Observations} gesucht, welche aus den Bildkoordinaten $(x_n,y_n)$ sowie einem Feature Vector $f_n$ bestehen. Der Feature Vektor ist dabei entweder die Intensität des Pixels oder ein berechneter Deskriptor, der aus den Intensitäten der lokalen Nachbarschaft des Pixels berechnet wird. Mithilfe dieser beiden Informationen wird ein Gitternetz generiert. Die eigentliche Berechnung der Disparität erfolgt mit Hilfe der \emph{Observations} auf deren jeweiliger Epipolarlinie unter Benutzung des \emph{Maximum a posteriori} (MAP) Verfahrens und wird für jedes Bild separat angewandt.\\

\noindent
Der von Tsin et al. \cite{tsin2003stereo} entwickelte \emph{Stereo Matching} Algorithmus kann zwischen Reflexion und Transparenz unterscheiden.
Grundlage ist das Reflexionsmodell nach Hero (Abbildung \ref{fig:reflection_models}), wonach der Einfallswinkel eines reflektierten Lichtstrahls (gemessen bezüglich der Oberflächennormale) gleich dem Ausfallwinkel ist. Somit ist die Position der Reflexion eines Szenepunktes unabhängig vom aktuellen Sichtfenster, das sich diese an einer festen Position hinter dem Reflektor befindet. Daher kann die Abhängigkeit eines Szenepunktes von seiner Reflexion ignoriert werden. Tsin et al. beschreiben ein geschichtetes Modell in dem das reflektierende Objekt als vordere Ebene $I_0$ und die Reflexion selber als hintere Ebene $I_1$ dargestellt ist (Abbildung \ref{fig:reflection}(b)).\\

\begin{figure}[h]
  	\centering
	\begin{tabular}{m{6.5cm} m{6.5cm}}
	\includegraphics[width=6.5cm]{img/reflection_model_hero.pdf}
	\begin{center} \small (a) \end{center}
	&
	\includegraphics[width=6.5cm]{img/reflection_model_layers.pdf}
	\begin{center} \small (b) \end{center}
	\end{tabular}
\caption{(a) beschreibt die physikalische Reflexion nach Hero, (b) das von Tsin et al. verwendete geschichtete Modell}
\label{fig:reflection_models}
\end{figure}

\noindent
Die aufgenommenen Bilder sind demnach eine Zusammensetzung beider Ebenen. Selbiges gilt für nicht planare Flächen sowie Durchsichtigkeit solange Translation und Rotation zwischen den einzelnen Bildern gering sind. Während des Algorithmus werden die relativen Bewegungen der Ebenen zwischen aufeinanderfolgenden Frames anhand der verschiedenen Tiefen analysiert.\\

\noindent
Ein weiterer, bereits in Abschnitt \ref{sec:existing_conflicts} angeschnittener Punkt ist die Erkennung des Bodens als Hindernis. Dabei stellt sich die Frage, ob eine Erkennung als Hindernis gewünscht ist oder nicht. Im Falle einer geringen Flughöhe wird der Boden ebenfalls als Hindernis erkannt. Dies kann einerseits eine Absicherung dafür sein, dass die durch interne Sensoren ermittelte Flughöhe korrekt ist, andrerseits kann es dazu führen, dass der Algorithmus zur Vermeidung von Hindernissen versucht, diesen zu  umgehen. Dies könnte unter Umständen dazu führen, dass das UAS eine Flughöhe annimmt, welche über der durch die Umgebung gegebenen maximalen Flughöhe liegt. Eine mögliche Lösung wäre die Erkennung des Bodens in Abhängigkeit der aktuellen Höhe zu gestalten. Befindet sich das UAS in einer Höhe in welcher der Boden als Hindernis erkannt wird, so wird dieser nicht weiter betrachtet. 

% TODO gucken in welchem paper das war